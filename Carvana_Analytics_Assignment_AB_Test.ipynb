{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Fz77x27xvox_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ipython-sql in /Users/ivan/miniforge3/lib/python3.9/site-packages (0.4.0)\n",
            "Requirement already satisfied: prettytable<1 in /Users/ivan/miniforge3/lib/python3.9/site-packages (from ipython-sql) (0.7.2)\n",
            "Requirement already satisfied: sqlparse in /Users/ivan/miniforge3/lib/python3.9/site-packages (from ipython-sql) (0.4.2)\n",
            "Requirement already satisfied: ipython-genutils>=0.1.0 in /Users/ivan/miniforge3/lib/python3.9/site-packages (from ipython-sql) (0.2.0)\n",
            "Requirement already satisfied: sqlalchemy>=0.6.7 in /Users/ivan/miniforge3/lib/python3.9/site-packages (from ipython-sql) (1.4.31)\n",
            "Requirement already satisfied: ipython>=1.0 in /Users/ivan/miniforge3/lib/python3.9/site-packages (from ipython-sql) (7.30.1)\n",
            "Requirement already satisfied: six in /Users/ivan/miniforge3/lib/python3.9/site-packages (from ipython-sql) (1.15.0)\n",
            "Requirement already satisfied: decorator in /Users/ivan/miniforge3/lib/python3.9/site-packages (from ipython>=1.0->ipython-sql) (5.1.0)\n",
            "Requirement already satisfied: pygments in /Users/ivan/miniforge3/lib/python3.9/site-packages (from ipython>=1.0->ipython-sql) (2.10.0)\n",
            "Requirement already satisfied: appnope in /Users/ivan/miniforge3/lib/python3.9/site-packages (from ipython>=1.0->ipython-sql) (0.1.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /Users/ivan/miniforge3/lib/python3.9/site-packages (from ipython>=1.0->ipython-sql) (59.4.0)\n",
            "Requirement already satisfied: matplotlib-inline in /Users/ivan/miniforge3/lib/python3.9/site-packages (from ipython>=1.0->ipython-sql) (0.1.3)\n",
            "Requirement already satisfied: pickleshare in /Users/ivan/miniforge3/lib/python3.9/site-packages (from ipython>=1.0->ipython-sql) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /Users/ivan/miniforge3/lib/python3.9/site-packages (from ipython>=1.0->ipython-sql) (5.1.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /Users/ivan/miniforge3/lib/python3.9/site-packages (from ipython>=1.0->ipython-sql) (4.8.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /Users/ivan/miniforge3/lib/python3.9/site-packages (from ipython>=1.0->ipython-sql) (0.18.1)\n",
            "Requirement already satisfied: backcall in /Users/ivan/miniforge3/lib/python3.9/site-packages (from ipython>=1.0->ipython-sql) (0.2.0)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Users/ivan/miniforge3/lib/python3.9/site-packages (from ipython>=1.0->ipython-sql) (3.0.24)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/ivan/miniforge3/lib/python3.9/site-packages (from jedi>=0.16->ipython>=1.0->ipython-sql) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /Users/ivan/miniforge3/lib/python3.9/site-packages (from pexpect>4.3->ipython>=1.0->ipython-sql) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /Users/ivan/miniforge3/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=1.0->ipython-sql) (0.2.5)\n",
            " * sqlite://\n",
            "(sqlite3.OperationalError) near \"persist\": syntax error\n",
            "[SQL: persist users]\n",
            "(Background on this error at: https://sqlalche.me/e/14/e3q8)\n",
            " * sqlite://\n",
            "(sqlite3.OperationalError) near \"persist\": syntax error\n",
            "[SQL: persist searches]\n",
            "(Background on this error at: https://sqlalche.me/e/14/e3q8)\n",
            " * sqlite://\n",
            "(sqlite3.OperationalError) near \"persist\": syntax error\n",
            "[SQL: persist vdps]\n",
            "(Background on this error at: https://sqlalche.me/e/14/e3q8)\n",
            " * sqlite://\n",
            "(sqlite3.OperationalError) near \"persist\": syntax error\n",
            "[SQL: persist sales]\n",
            "(Background on this error at: https://sqlalche.me/e/14/e3q8)\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Welcome to the Carvana Analytics experiment evaluation assignment!\n",
        "\n",
        "In this exercise you will be presented with data from a fictional A/B test and asked to evaluate and interpret the results.\n",
        "\n",
        "Carvana.com is testing a new feature and is running an A/B test to quantify the impact it has on different types of users. Before their first search, a user is bucketed into one of two treatment groups:\n",
        "  Test - the user is exposed to the new feature\n",
        "  Control - the user is not exposed to the new feature\n",
        "\n",
        "All bucketed users can be found in the \"users\" dataframe/table. This data contains:\n",
        "  user_id\n",
        "  region\n",
        "  treatment\n",
        "\n",
        "All searches done by users can be found in the \"searches\" dataframe/table. This data contains:\n",
        "  user_id\n",
        "  event_date_time\n",
        "  device_type\n",
        "  event_id\n",
        "\n",
        "As users progress through the website, their searches may return vehicles they are interested in. Clicking on a vehicle takes them to a vehicle detail page, or VDP. This is a strong sign of engagement.\n",
        "\n",
        "All VDPs done by users can be found in the \"vdps\" dataframe/table. This data contains:\n",
        "  user_id\n",
        "  event_date_time\n",
        "  device_type\n",
        "  event_id\n",
        "\n",
        "A user can purchase a vehicle driectly from a VDP. This constitues a sale and a conversion for the user.\n",
        "\n",
        "All sales completed by users can be found in the \"sales\" dataframe/table. This data contains:\n",
        "  user_id\n",
        "  event_date_time\n",
        "  device_type\n",
        "  event_id\n",
        "\n",
        "Use the four datasets described to accomplish the following tasks:\n",
        "\n",
        "1) Evaluate the effect of the new feature on engagement (searches and VDPs) and conversion using statistical significance where applicable\n",
        "2) Summarize and highlight insights (or issues) in user behavior across various segments\n",
        "3) Provide a recommendation on whether or not to permanently deploy the feature to all users, some users, or no users\n",
        "\n",
        "Clone or copy this notebook and run this cell to begin. Once you do so you will be able to work with the data in python and/or write sql queries against the data (see example cells below)\n",
        "\n",
        "When submitting the assignment please provide a link to your notebook in addition to your typed respones to the items above in .pdf format.\n",
        "\n",
        "The estimated time for this exercise is 3-4 hours. Please submit your answers to your recruiting coordinator. Good luck!\n",
        "\n",
        "''' \n",
        "\n",
        "################################\n",
        "#### do not alter this code ####\n",
        "################################\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "! pip install ipython-sql\n",
        "\n",
        "users_path=\"https://s3-us-west-2.amazonaws.com/carvana-analytics-assignment/users.csv\"\n",
        "searches_path=\"https://s3-us-west-2.amazonaws.com/carvana-analytics-assignment/searches.csv\"\n",
        "vdps_path=\"https://s3-us-west-2.amazonaws.com/carvana-analytics-assignment/vdps.csv\"\n",
        "sales_path=\"https://s3-us-west-2.amazonaws.com/carvana-analytics-assignment/sales.csv\"\n",
        "\n",
        "users=pd.read_csv(users_path)\n",
        "searches=pd.read_csv(searches_path)\n",
        "vdps=pd.read_csv(vdps_path)\n",
        "sales=pd.read_csv(sales_path)\n",
        "\n",
        "%load_ext sql\n",
        "%sql sqlite://\n",
        "\n",
        "%sql persist users\n",
        "%sql persist searches\n",
        "%sql persist vdps\n",
        "%sql persist sales\n",
        "\n",
        "################################\n",
        "#### do not alter this code ####\n",
        "################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "#imports that I will need \n",
        "# ttest_ind is in the independent t-test for comparing means\n",
        "# datetime is for managing datetime strings given\n",
        "from scipy.stats import ttest_ind, chi2_contingency, chi2\n",
        "from datetime import datetime as dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jIgSt1elKk2h"
      },
      "outputs": [],
      "source": [
        "# lets group the events to their appropriate user \n",
        "searches_by_user = {k: v for k, v in searches.groupby('user_id')}\n",
        "vdps_by_user = {k: v for k, v in vdps.groupby('user_id')}\n",
        "sales_by_user = {k: v for k, v in sales.groupby('user_id')}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rBBNjBpZNu4v"
      },
      "outputs": [],
      "source": [
        "# This function is the main data cleaning and rearranging step\n",
        "\n",
        "def attribute_actions_to_user(users_df, searches_by_user, vdps_by_user, sales_by_user):\n",
        "    \"\"\"\n",
        "    Attributes the events (i.e., searches, vdps, sales) to the appropriate user in the study.\n",
        "    Computes KPIs for each user (duration, count_searches, count_vdps, sale).\n",
        "    each user entry takes the form of a dict with the keys, 'user_id', 'treatment', 'region',\n",
        "    'device_type', 'duration', 'count_searches', 'count_vdps', 'sale'.\n",
        "    \n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    users_df : pd.DataFrame object\n",
        "        Dataframe of the users in the study.\n",
        "        Must contain the columns 'user_id', 'treatment', 'region', 'event_date_time'\n",
        "\n",
        "    searches_by_user : pd.GroubyObject\n",
        "        Groupby object of all the searches performed, grouped by 'user_id'\n",
        "        Must contain the columns 'user_id', 'device_type', 'event_date_time'\n",
        "\n",
        "    vdps_by_user : pd.GroubyObject\n",
        "        Groupby object of all the VDP pages visited, grouped by 'user_id'\n",
        "        Must contain the columns 'user_id', 'device_type', 'event_date_time'\n",
        "\n",
        "    sales_by_user : pd.GroubyObject\n",
        "        Groupby object of all the sales made, grouped by 'user_id'\n",
        "        Must contain the columns 'user_id', 'device_type', 'event_date_time'\n",
        "\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "        summary_df : pd.DataFrame object\n",
        "            summary KPIs/statistics for each user. Each columns descriptions are\n",
        "            'user_id': int \n",
        "                unique identifier for a user in the study\n",
        "            'treatment': str\n",
        "                'Test' or 'Control'\n",
        "            'region': str \n",
        "                region in ['New England', 'Southwest', 'Southeast', 'Midwest', 'Pacific Northwest']\n",
        "            'device_type': str\n",
        "                'Desktop' or 'Mobile' \n",
        "            'duration': int\n",
        "                number of minutes of customer journey\n",
        "            'count_searches': int\n",
        "                number of searches performed\n",
        "            'count_vdps': int\n",
        "                number of vdps pages visited\n",
        "            'sale': int\n",
        "                number of sales completed\n",
        "    Raises\n",
        "    ------\n",
        "        None\n",
        "    \"\"\"\n",
        "    summary_dicts = []\n",
        "    def get_device(searches_by_user, vdps_by_user, sales_by_user, user_id):\n",
        "        \"\"\"\n",
        "        gets a device name from the initial actions of a user.\n",
        "        outputs the device as the device where the sale was made.\n",
        "        \"\"\"        \n",
        "        if user_id in searches_by_user.keys():\n",
        "            device_name = searches_by_user[user_id]['device_type'].iloc[0]\n",
        "        elif user_id in vdps_by_user.keys():\n",
        "            device_name = vdps_by_user[user_id]['device_type'].iloc[0]\n",
        "        elif user_id in sales_by_user.keys():\n",
        "            device_name = sales_by_user[user_id]['device_type'].iloc[0]\n",
        "        else:\n",
        "            device_name = None\n",
        "        return device_name\n",
        "    \n",
        "    def get_duration(searches_by_user, vdps_by_user, sales_by_user, user_id):\n",
        "        \"\"\"\n",
        "        gets the amount of time in minutes that a user spent on their\n",
        "        customer journey.\n",
        "        \"\"\"\n",
        "        _start_se, _start_vd, _start_sa = None, None, None\n",
        "        _end_se, _end_vd, _end_sa = None, None, None\n",
        "        \n",
        "        if user_id in searches_by_user.keys():\n",
        "            _start_se = min(searches_by_user[user_id]['event_date_time'])\n",
        "            _end_se = max(searches_by_user[user_id]['event_date_time'])\n",
        "        \n",
        "        if user_id in vdps_by_user.keys():\n",
        "            _start_vd = min(vdps_by_user[user_id]['event_date_time'])\n",
        "            _end_vd = max(vdps_by_user[user_id]['event_date_time'])\n",
        "        \n",
        "        if user_id in sales_by_user.keys():\n",
        "            _start_sa = min(sales_by_user[user_id]['event_date_time'])\n",
        "            _end_sa = max(sales_by_user[user_id]['event_date_time'])\n",
        "\n",
        "        \n",
        "        j_start = min([_start for _start in [_start_se, _start_vd, _start_sa] if _start != None ])\n",
        "        \n",
        "        j_end =  max([_end for _end in [_end_se, _end_vd, _end_sa] if _end != None ])\n",
        "\n",
        "        j_start_dt = dt.strptime(j_start, '%m/%d/%y %H:%M') if '//' in str(j_start) else dt.strptime(j_start, '%Y-%m-%d %H:%M:%S')\n",
        "        j_end_dt = dt.strptime(j_end, '%m/%d/%y %H:%M') if '//' in str(j_end) else dt.strptime(j_end, '%Y-%m-%d %H:%M:%S')\n",
        "            \n",
        "        duration = j_end_dt - j_start_dt\n",
        "        mins = duration.total_seconds()/60\n",
        "        return mins\n",
        "\n",
        "    def get_action_counts(searches_by_user, vdps_by_user, sales_by_user, user_id):\n",
        "        \"\"\"\n",
        "        gets a count of the three types of actions(searches, vdp views, sales) that a user performed. \n",
        "        outputs the counts into 3 item list.\n",
        "        \"\"\"\n",
        "        _count_searches, _count_vdps, _count_sales = 0, 0, 0\n",
        "        if user_id in searches_by_user.keys():\n",
        "            _count_searches = len(searches_by_user[user_id])\n",
        "            \n",
        "        if user_id in vdps_by_user.keys():\n",
        "            _count_vdps = len(vdps_by_user[user_id])\n",
        "\n",
        "        if user_id in sales_by_user.keys():\n",
        "            _count_sales = len(sales_by_user[user_id])\n",
        "\n",
        "        counts = [_count_searches, _count_vdps, _count_sales]\n",
        "        return counts\n",
        "\n",
        "    for row in users_df.itertuples():\n",
        "        device = get_device(searches_by_user, vdps_by_user, sales_by_user, row.user_id)\n",
        "        mins = get_duration(searches_by_user, vdps_by_user, sales_by_user, row.user_id)\n",
        "        actioncounts = get_action_counts(searches_by_user, vdps_by_user, sales_by_user, row.user_id)\n",
        "        summary_dicts.append(\n",
        "                {\n",
        "                    'user_id':row.user_id,\n",
        "                    'treatment': row.treatment,\n",
        "                    'region':row.region,\n",
        "                    'device_type': device, \n",
        "                    'duration':mins,\n",
        "                    'count_searches':actioncounts[0],\n",
        "                    'count_vdps': actioncounts[1],\n",
        "                    'sale':actioncounts[2]\n",
        "                }\n",
        "            )\n",
        "    summary_df = pd.DataFrame(summary_dicts)\n",
        "    return summary_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "            user_id      duration  count_searches    count_vdps          sale\n",
            "count  13000.000000  13000.000000    13000.000000  13000.000000  13000.000000\n",
            "mean    7499.500000     16.621123        5.477846      1.364462      0.103077\n",
            "std     3752.921085     35.156267        9.040641      8.854193      0.304071\n",
            "min     1000.000000      0.000000        1.000000      0.000000      0.000000\n",
            "25%     4249.750000      3.100000        4.000000      0.000000      0.000000\n",
            "50%     7499.500000      4.800000        5.000000      1.000000      0.000000\n",
            "75%    10749.250000      7.216667        7.000000      2.000000      0.000000\n",
            "max    13999.000000    131.850000     1000.000000   1000.000000      1.000000\n",
            "   user_id treatment       region device_type   duration  count_searches  \\\n",
            "0     1000      Test    Southwest      Mobile   8.833333              10   \n",
            "1     1001      Test  New England     Desktop   4.366667               5   \n",
            "2     1002      Test    Southeast      Mobile   3.550000               4   \n",
            "3     1003      Test    Southwest     Desktop   8.100000               9   \n",
            "4     1004      Test    Southeast     Desktop  13.000000              12   \n",
            "\n",
            "   count_vdps  sale  \n",
            "0           2     0  \n",
            "1           3     0  \n",
            "2           0     0  \n",
            "3           6     0  \n",
            "4           2     0  \n"
          ]
        }
      ],
      "source": [
        "# lets see what user level base analytics show\n",
        "user_summary_df = attribute_actions_to_user(users, searches_by_user, vdps_by_user, sales_by_user)\n",
        "print(user_summary_df.describe())\n",
        "print(user_summary_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_user_level_t_stats(user_data):\n",
        "    \"\"\"\n",
        "    Generates summary data table for independent students t-test for the five\n",
        "    KPIs being tested in the AB study. The user level KPIs are:\n",
        "        'duration': int\n",
        "            number of minutes of customer journey\n",
        "        'count_searches': int\n",
        "            number of searches performed\n",
        "        'count_vdps': int\n",
        "            number of vdps pages visited\n",
        "        'sale': int\n",
        "            number of sales completed\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    user_data : pd.DataFrame object\n",
        "        Summary user data generated from the function 'attribute_actions_to_user' earlier in this\n",
        "        notebook. Must contain the columns, 'duration', 'count_searches', 'count_vdps', 'sale'. \n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    tstat_df : pd.DataFrame object\n",
        "        Dataframe object containing the summary student t-test info for each KPI. \n",
        "        columns include:\n",
        "            'meanTreat': float\n",
        "                the mean KPI for the Treatment group\n",
        "            'meanCtrl': float\n",
        "                the mean KPI for the Control group\n",
        "            'Tstat': float\n",
        "                the t-test statistic calculated from the data\n",
        "            'pvalue': float\n",
        "                the corresponding p-value for the given test-statistic and degress of freedom\n",
        "            'RejH0@0.05': bool\n",
        "                True if the null hypothesis can be rejected at alpha=0.05.\n",
        "                False if the null hypothesis cannot be reject at alpha=0.05.\n",
        "    Raises\n",
        "    ------\n",
        "        None\n",
        "    \n",
        "    Notes\n",
        "    -----\n",
        "        Depends on scipy.stats.ttest_ind \n",
        "    \"\"\"\n",
        "    tstat_dicts = []\n",
        "    by_treatment = list(user_data.groupby('treatment'))\n",
        "    KPIs = ['duration', 'count_searches', 'count_vdps', 'sale']\n",
        "    for i in KPIs:\n",
        "        ttestResultStat, ttestResultpval = ttest_ind(by_treatment[1][1][i], by_treatment[0][1][i])\n",
        "        _entry = {\n",
        "        'KPI': i,\n",
        "        'meanTest': by_treatment[1][1][i].mean(),\n",
        "        'meanCtrl': by_treatment[0][1][i].mean(),\n",
        "        'Tstat': ttestResultStat,\n",
        "        'pvalue': ttestResultpval,\n",
        "        'RejH0@0.05': True if ttestResultpval < 0.05 else False\n",
        "        }\n",
        "        tstat_dicts.append(_entry)\n",
        "    tstat_df = pd.DataFrame(tstat_dicts)\n",
        "    print(tstat_df)\n",
        "    return tstat_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chisquare test\n",
        "def chi_square_test(user_data,attribute):\n",
        "    \"\"\"\n",
        "    Function for performing Chi-Squared test on sales data because the sales data\n",
        "    is a categorical value (0, 1). \n",
        "    I borrow much of the original Chi-Squared implementation from\n",
        "    https://machinelearningmastery.com/chi-squared-test-for-machine-learning/\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    user_data : pd.Dataframe object\n",
        "        Contains the user_data from the function 'attribute_actions_to_user'.\n",
        "        Must contain the columns 'sale', 'region', 'device_type', and 'treatment'.\n",
        "    \n",
        "    attribute : str\n",
        "        attribute to be testing sales against. Must be 'region', 'device_type', or 'treatment'.\n",
        "    \n",
        "    Returns \n",
        "    -------\n",
        "        None\n",
        "    \n",
        "    Notes\n",
        "    -----\n",
        "        Contains print statements for \n",
        "            1. contingency table\n",
        "            2. Chi-Squared critical value calculation\n",
        "            3. conditional statement on whether to reject H0 @ alpha = 0.05 for critical value\n",
        "            4. Chi-Squared p-value calculation\n",
        "            5. conditional statement on whether to reject H0 @ alpha = 0.05 for p-value\n",
        "    \"\"\"\n",
        "    # we can try contingnegyt tables to look at the data for sales\n",
        "    ctable = pd.crosstab(index=user_data[attribute], columns=user_data['sale'], margins=True)\n",
        "    print(ctable)\n",
        "    if attribute == 'region':\n",
        "        regions = ['New England', 'Southwest', 'Southeast', 'Midwest', 'Pacific Northwest']\n",
        "        ctable_O = [[ctable.at[region, 0] , ctable.at[region, 1]] for region in regions]\n",
        "    elif attribute == 'device_type':\n",
        "        ctable_O = [[ctable.at[device, 0] , ctable.at[device, 1]] for device in ['Desktop', 'Mobile']]\n",
        "    elif attribute == 'treatment':\n",
        "        ctable_O = [[ctable.at[treat, 0] , ctable.at[treat, 1]] for treat in ['Control', 'Test']]\n",
        "    else:\n",
        "        print(\"unknown device type\")\n",
        "    \n",
        "    stat, p, dof, expected = chi2_contingency(ctable_O)\n",
        "    # print('dof=%d' % dof)\n",
        "    # print(expected)\n",
        "    # interpret test-statistic\n",
        "    prob = 0.95\n",
        "    critical = chi2.ppf(prob, dof)\n",
        "    print('probability=%.3f, critical=%.3f, stat=%.3f' % (prob, critical, stat))\n",
        "    if abs(stat) >= critical:\n",
        "        print('Dependent (reject H0)')\n",
        "    else:\n",
        "        print('Independent (fail to reject H0)')\n",
        "    # interpret p-value\n",
        "    alpha = 1.0 - prob\n",
        "    print('significance=%.3f, p=%.3f' % (alpha, p))\n",
        "    if p <= alpha:\n",
        "        print('Dependent (reject H0)')\n",
        "    else:\n",
        "        print('Independent (fail to reject H0)')\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              KPI   meanTest   meanCtrl     Tstat    pvalue  RejH0@0.05\n",
            "0        duration  16.679894  16.563516  0.188700  0.850331       False\n",
            "1  count_searches   5.620047   5.338462  1.775687  0.075808       False\n",
            "2      count_vdps   1.287024   1.440366 -0.987257  0.323535       False\n",
            "3            sale   0.101632   0.104494 -0.536508  0.591617       False\n",
            "sale           0     1    All\n",
            "treatment                    \n",
            "Control     5879   686   6565\n",
            "Test        5781   654   6435\n",
            "All        11660  1340  13000\n",
            "probability=0.950, critical=3.841, stat=0.258\n",
            "Independent (fail to reject H0)\n",
            "significance=0.050, p=0.612\n",
            "Independent (fail to reject H0)\n"
          ]
        }
      ],
      "source": [
        "### no grouping \n",
        "get_user_level_t_stats(user_summary_df)\n",
        "chi_square_test(user_summary_df, 'treatment')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Desktop\n",
            "              KPI   meanTest   meanCtrl     Tstat    pvalue  RejH0@0.05\n",
            "0        duration  17.311658  17.162309  0.179144  0.857830       False\n",
            "1  count_searches   5.424602   5.587370 -0.596829  0.550640       False\n",
            "2      count_vdps   1.382789   1.589243 -0.764724  0.444460       False\n",
            "3            sale   0.108713   0.108643  0.009694  0.992266       False\n",
            "sale          0    1   All\n",
            "treatment                 \n",
            "Control    3331  406  3737\n",
            "Test       3304  403  3707\n",
            "All        6635  809  7444\n",
            "probability=0.950, critical=3.841, stat=0.000\n",
            "Independent (fail to reject H0)\n",
            "significance=0.050, p=1.000\n",
            "Independent (fail to reject H0)\n",
            "Mobile\n",
            "              KPI   meanTest   meanCtrl      Tstat        pvalue  RejH0@0.05\n",
            "0        duration  15.821408  15.772254   0.053815  9.570847e-01       False\n",
            "1  count_searches   5.885630   5.009547  13.761598  2.138779e-42        True\n",
            "2      count_vdps   1.156891   1.243635  -2.493909  1.266336e-02        True\n",
            "3            sale   0.092009   0.099010  -0.887252  3.749819e-01       False\n",
            "sale          0    1   All\n",
            "treatment                 \n",
            "Control    2548  280  2828\n",
            "Test       2477  251  2728\n",
            "All        5025  531  5556\n",
            "probability=0.950, critical=3.841, stat=0.708\n",
            "Independent (fail to reject H0)\n",
            "significance=0.050, p=0.400\n",
            "Independent (fail to reject H0)\n"
          ]
        }
      ],
      "source": [
        "### grouping by device\n",
        "for item, group in user_summary_df.groupby('device_type'):\n",
        "    print(item)\n",
        "    get_user_level_t_stats(group)\n",
        "    # we can try contingnegyt tables to look at the data for sales\n",
        "    chi_square_test(group, 'treatment')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Midwest\n",
            "              KPI   meanTest   meanCtrl      Tstat        pvalue  RejH0@0.05\n",
            "0        duration  12.037316  18.840856  -5.231506  1.819126e-07        True\n",
            "1  count_searches   5.807571   5.773913   0.357704  7.205947e-01       False\n",
            "2      count_vdps   0.820978   1.429249 -11.643332  1.469912e-30        True\n",
            "3            sale   0.059148   0.120158  -5.403799  7.133412e-08        True\n",
            "sale          0    1   All\n",
            "treatment                 \n",
            "Control    1113  152  1265\n",
            "Test       1193   75  1268\n",
            "All        2306  227  2533\n",
            "probability=0.950, critical=3.841, stat=28.148\n",
            "Dependent (reject H0)\n",
            "significance=0.050, p=0.000\n",
            "Dependent (reject H0)\n",
            "New England\n",
            "              KPI   meanTest   meanCtrl     Tstat    pvalue  RejH0@0.05\n",
            "0        duration  15.089132  12.324923  2.152508  0.031450        True\n",
            "1  count_searches   4.537441   4.198157  4.139686  0.000036        True\n",
            "2      count_vdps   1.164587   1.013057  3.379553  0.000737        True\n",
            "3            sale   0.096724   0.075269  1.946521  0.051700       False\n",
            "sale          0    1   All\n",
            "treatment                 \n",
            "Control    1204   98  1302\n",
            "Test       1158  124  1282\n",
            "All        2362  222  2584\n",
            "probability=0.950, critical=3.841, stat=3.518\n",
            "Independent (fail to reject H0)\n",
            "significance=0.050, p=0.061\n",
            "Independent (fail to reject H0)\n",
            "Pacific Northwest\n",
            "              KPI   meanTest   meanCtrl     Tstat        pvalue  RejH0@0.05\n",
            "0        duration  19.530196  17.502694  1.706080  8.807194e-02       False\n",
            "1  count_searches   5.845524   5.278449  7.620644  3.142843e-14        True\n",
            "2      count_vdps   1.504859   1.305136  4.823331  1.465183e-06        True\n",
            "3            sale   0.124297   0.111782  1.217515  2.234812e-01       False\n",
            "sale          0    1   All\n",
            "treatment                 \n",
            "Control    1764  222  1986\n",
            "Test       1712  243  1955\n",
            "All        3476  465  3941\n",
            "probability=0.950, critical=3.841, stat=1.365\n",
            "Independent (fail to reject H0)\n",
            "significance=0.050, p=0.243\n",
            "Independent (fail to reject H0)\n",
            "Southeast\n",
            "              KPI   meanTest   meanCtrl     Tstat        pvalue  RejH0@0.05\n",
            "0        duration  16.807598  17.478606 -0.422069  6.730211e-01       False\n",
            "1  count_searches   6.239107   5.516192  6.750468  1.935280e-11        True\n",
            "2      count_vdps   1.486716   1.428852  0.962701  3.358163e-01       False\n",
            "3            sale   0.097768   0.109912 -0.879296  3.793488e-01       False\n",
            "sale          0    1   All\n",
            "treatment                 \n",
            "Control     907  112  1019\n",
            "Test        849   92   941\n",
            "All        1756  204  1960\n",
            "probability=0.950, critical=3.841, stat=0.649\n",
            "Independent (fail to reject H0)\n",
            "significance=0.050, p=0.421\n",
            "Independent (fail to reject H0)\n",
            "Southwest\n",
            "              KPI   meanTest   meanCtrl     Tstat    pvalue  RejH0@0.05\n",
            "0        duration  18.938372  16.402518  1.552003  0.120821       False\n",
            "1  count_searches   5.748231   6.216516 -0.463937  0.642744       False\n",
            "2      count_vdps   1.422649   2.297080 -0.866245  0.386461       False\n",
            "3            sale   0.121335   0.102719  1.313832  0.189055       False\n",
            "sale          0    1   All\n",
            "treatment                 \n",
            "Control     891  102   993\n",
            "Test        869  120   989\n",
            "All        1760  222  1982\n",
            "probability=0.950, critical=3.841, stat=1.544\n",
            "Independent (fail to reject H0)\n",
            "significance=0.050, p=0.214\n",
            "Independent (fail to reject H0)\n"
          ]
        }
      ],
      "source": [
        "#grouping by region\n",
        "for item, group in user_summary_df.groupby('region'):\n",
        "    print(item)\n",
        "    get_user_level_t_stats(group)\n",
        "    # we can try contingnegyt tables to look at the data for sales\n",
        "    chi_square_test(group, 'treatment')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Desktop Midwest\n",
            "              KPI   meanTest   meanCtrl     Tstat    pvalue  RejH0@0.05\n",
            "0        duration  17.269652  18.508367 -0.648065  0.517046       False\n",
            "1  count_searches   5.675599   5.562416  0.919699  0.357883       False\n",
            "2      count_vdps   1.468265   1.318121  2.160228  0.030918        True\n",
            "3            sale   0.105783   0.118121 -0.744999  0.456393       False\n",
            "sale          0    1   All\n",
            "treatment                 \n",
            "Control     657   88   745\n",
            "Test        634   75   709\n",
            "All        1291  163  1454\n",
            "probability=0.950, critical=3.841, stat=0.439\n",
            "Independent (fail to reject H0)\n",
            "significance=0.050, p=0.508\n",
            "Independent (fail to reject H0)\n",
            "Desktop New England\n",
            "              KPI   meanTest   meanCtrl     Tstat    pvalue  RejH0@0.05\n",
            "0        duration  14.692739  13.607519  0.629399  0.529184       False\n",
            "1  count_searches   4.322497   4.317744  0.044516  0.964499       False\n",
            "2      count_vdps   1.106632   1.024759  1.446428  0.148267       False\n",
            "3            sale   0.094928   0.085282  0.650490  0.515476       False\n",
            "sale          0    1   All\n",
            "treatment                 \n",
            "Control     665   62   727\n",
            "Test        696   73   769\n",
            "All        1361  135  1496\n",
            "probability=0.950, critical=3.841, stat=0.314\n",
            "Independent (fail to reject H0)\n",
            "significance=0.050, p=0.575\n",
            "Independent (fail to reject H0)\n",
            "Desktop Pacific Northwest\n",
            "              KPI   meanTest   meanCtrl     Tstat    pvalue  RejH0@0.05\n",
            "0        duration  18.961375  17.529314  0.917554  0.358951       False\n",
            "1  count_searches   5.638609   5.486603  1.557587  0.119472       False\n",
            "2      count_vdps   1.483074   1.364736  2.167821  0.030277        True\n",
            "3            sale   0.120769   0.110631  0.751538  0.452408       False\n",
            "sale          0    1   All\n",
            "treatment                 \n",
            "Control    1029  128  1157\n",
            "Test        961  132  1093\n",
            "All        1990  260  2250\n",
            "probability=0.950, critical=3.841, stat=0.470\n",
            "Independent (fail to reject H0)\n",
            "significance=0.050, p=0.493\n",
            "Independent (fail to reject H0)\n",
            "Desktop Southeast\n",
            "              KPI   meanTest   meanCtrl     Tstat    pvalue  RejH0@0.05\n",
            "0        duration  16.786915  19.473405 -1.241337  0.214741       False\n",
            "1  count_searches   6.014210   5.876565  0.962983  0.335764       False\n",
            "2      count_vdps   1.456483   1.554562 -1.200240  0.230300       False\n",
            "3            sale   0.099467   0.125224 -1.366162  0.172162       False\n",
            "sale         0    1   All\n",
            "treatment                \n",
            "Control    489   70   559\n",
            "Test       507   56   563\n",
            "All        996  126  1122\n",
            "probability=0.950, critical=3.841, stat=1.617\n",
            "Independent (fail to reject H0)\n",
            "significance=0.050, p=0.203\n",
            "Independent (fail to reject H0)\n",
            "Desktop Southwest\n",
            "              KPI   meanTest   meanCtrl     Tstat    pvalue  RejH0@0.05\n",
            "0        duration  18.247120  16.916393  0.615690  0.538224       False\n",
            "1  count_searches   5.605585   7.220401 -0.907933  0.364109       False\n",
            "2      count_vdps   1.383944   3.213115 -1.026386  0.304931       False\n",
            "3            sale   0.116928   0.105647  0.599955  0.548658       False\n",
            "sale         0    1   All\n",
            "treatment                \n",
            "Control    491   58   549\n",
            "Test       506   67   573\n",
            "All        997  125  1122\n",
            "probability=0.950, critical=3.841, stat=0.256\n",
            "Independent (fail to reject H0)\n",
            "significance=0.050, p=0.613\n",
            "Independent (fail to reject H0)\n",
            "Mobile Midwest\n",
            "              KPI  meanTest   meanCtrl      Tstat         pvalue  RejH0@0.05\n",
            "0        duration  5.400954  19.317212  -8.717685   1.057279e-17        True\n",
            "1  count_searches  5.974955   6.076923  -0.703667   4.817921e-01       False\n",
            "2      count_vdps  0.000000   1.588462 -25.453968  2.804586e-112        True\n",
            "3            sale  0.000000   0.123077  -8.849334   3.545292e-18        True\n",
            "sale          0   1   All\n",
            "treatment                \n",
            "Control     456  64   520\n",
            "Test        559   0   559\n",
            "All        1015  64  1079\n",
            "probability=0.950, critical=3.841, stat=70.949\n",
            "Dependent (reject H0)\n",
            "significance=0.050, p=0.000\n",
            "Dependent (reject H0)\n",
            "Mobile New England\n",
            "              KPI   meanTest   meanCtrl     Tstat        pvalue  RejH0@0.05\n",
            "0        duration  15.683333  10.703275  2.591053  9.696545e-03        True\n",
            "1  count_searches   4.859649   4.046957  6.413540  2.117517e-10        True\n",
            "2      count_vdps   1.251462   0.998261  3.481259  5.188973e-04        True\n",
            "3            sale   0.099415   0.062609  2.237454  2.545887e-02        True\n",
            "sale          0   1   All\n",
            "treatment                \n",
            "Control     539  36   575\n",
            "Test        462  51   513\n",
            "All        1001  87  1088\n",
            "probability=0.950, critical=3.841, stat=4.505\n",
            "Dependent (reject H0)\n",
            "significance=0.050, p=0.034\n",
            "Dependent (reject H0)\n",
            "Mobile Pacific Northwest\n",
            "              KPI   meanTest   meanCtrl     Tstat        pvalue  RejH0@0.05\n",
            "0        duration  20.251450  17.465541  1.518606  1.290487e-01       False\n",
            "1  count_searches   6.107889   4.987937  9.849164  2.718638e-22        True\n",
            "2      count_vdps   1.532483   1.221954  4.890449  1.101685e-06        True\n",
            "3            sale   0.128770   0.113390  0.968401  3.329827e-01       False\n",
            "sale          0    1   All\n",
            "treatment                 \n",
            "Control     735   94   829\n",
            "Test        751  111   862\n",
            "All        1486  205  1691\n",
            "probability=0.950, critical=3.841, stat=0.800\n",
            "Independent (fail to reject H0)\n",
            "significance=0.050, p=0.371\n",
            "Independent (fail to reject H0)\n",
            "Mobile Southeast\n",
            "              KPI   meanTest   meanCtrl     Tstat        pvalue  RejH0@0.05\n",
            "0        duration  16.838404  15.054493  0.764406  4.448411e-01       False\n",
            "1  count_searches   6.574074   5.078261  9.461463  2.994524e-20        True\n",
            "2      count_vdps   1.531746   1.276087  2.906577  3.750627e-03        True\n",
            "3            sale   0.095238   0.091304  0.194801  8.455964e-01       False\n",
            "sale         0   1  All\n",
            "treatment              \n",
            "Control    418  42  460\n",
            "Test       342  36  378\n",
            "All        760  78  838\n",
            "probability=0.950, critical=3.841, stat=0.006\n",
            "Independent (fail to reject H0)\n",
            "significance=0.050, p=0.940\n",
            "Independent (fail to reject H0)\n",
            "Mobile Southwest\n",
            "              KPI   meanTest   meanCtrl     Tstat        pvalue  RejH0@0.05\n",
            "0        duration  19.890505  15.767117  1.649638  9.938307e-02       False\n",
            "1  count_searches   5.944712   4.975225  5.900672  5.207763e-09        True\n",
            "2      count_vdps   1.475962   1.164414  3.636188  2.931926e-04        True\n",
            "3            sale   0.127404   0.099099  1.311075  1.901832e-01       False\n",
            "sale         0   1  All\n",
            "treatment              \n",
            "Control    400  44  444\n",
            "Test       363  53  416\n",
            "All        763  97  860\n",
            "probability=0.950, critical=3.841, stat=1.448\n",
            "Independent (fail to reject H0)\n",
            "significance=0.050, p=0.229\n",
            "Independent (fail to reject H0)\n"
          ]
        }
      ],
      "source": [
        "# grouping by device then region\n",
        "for device, data in user_summary_df.groupby('device_type'):\n",
        "    for region, device_data in data.groupby('region'):\n",
        "        print(device, region)\n",
        "        try:\n",
        "            tstat_df = get_user_level_t_stats(device_data)\n",
        "            # we can try contingnegyt tables to look at the data for sales\n",
        "            chi_square_test(device_data, 'treatment')\n",
        "        except Exception as e:\n",
        "            # this only happens because Mobile Midwest users\n",
        "            # were not shown the feature at all\n",
        "            print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "def three_way_anova(KPI, catVar1, catVar2, catVar3, dataset):\n",
        "    \"\"\"\n",
        "    quick function to perform a three way anova using statsmodels.api,\n",
        "    and statsmodels.formula.api.ols.\n",
        "    See https://www.statsmodels.org/stable/anova.html\n",
        "    \"\"\"\n",
        "    lm = ols('{} ~ C({}, Sum)*C({}, Sum)*C({}, Sum)'.format(KPI, catVar1, catVar2, catVar3), data=dataset).fit()\n",
        "    table = sm.stats.anova_lm(lm, typ=2)\n",
        "    print(table.to_markdown())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "duration\n",
            "                                                          sum_sq       df  \\\n",
            "C(treatment, Sum)                                   5.286280e+01      1.0   \n",
            "C(device_type, Sum)                                 6.339614e+03      1.0   \n",
            "C(region, Sum)                                      4.269443e+04      4.0   \n",
            "C(treatment, Sum):C(device_type, Sum)               2.822341e+01      1.0   \n",
            "C(treatment, Sum):C(region, Sum)                    4.008989e+04      4.0   \n",
            "C(device_type, Sum):C(region, Sum)                  1.658538e+04      4.0   \n",
            "C(treatment, Sum):C(device_type, Sum):C(region,...  3.099058e+04      4.0   \n",
            "Residual                                            1.592819e+07  12980.0   \n",
            "\n",
            "                                                           F        PR(>F)  \n",
            "C(treatment, Sum)                                   0.043078  8.355812e-01  \n",
            "C(device_type, Sum)                                 5.166197  2.304685e-02  \n",
            "C(region, Sum)                                      8.698000  5.232922e-07  \n",
            "C(treatment, Sum):C(device_type, Sum)               0.022999  8.794608e-01  \n",
            "C(treatment, Sum):C(region, Sum)                    8.167385  1.421353e-06  \n",
            "C(device_type, Sum):C(region, Sum)                  3.378886  9.036650e-03  \n",
            "C(treatment, Sum):C(device_type, Sum):C(region,...  6.313611  4.518767e-05  \n",
            "Residual                                                 NaN           NaN  \n",
            "count_searches\n",
            "                                                          sum_sq       df  \\\n",
            "C(treatment, Sum)                                   2.489969e+02      1.0   \n",
            "C(device_type, Sum)                                 1.656441e+01      1.0   \n",
            "C(region, Sum)                                      4.226029e+03      4.0   \n",
            "C(treatment, Sum):C(device_type, Sum)               8.130528e+02      1.0   \n",
            "C(treatment, Sum):C(region, Sum)                    5.201408e+02      4.0   \n",
            "C(device_type, Sum):C(region, Sum)                  5.536839e+02      4.0   \n",
            "C(treatment, Sum):C(device_type, Sum):C(region,...  5.548891e+02      4.0   \n",
            "Residual                                            1.055486e+06  12980.0   \n",
            "\n",
            "                                                            F        PR(>F)  \n",
            "C(treatment, Sum)                                    3.062076  8.016285e-02  \n",
            "C(device_type, Sum)                                  0.203703  6.517557e-01  \n",
            "C(region, Sum)                                      12.992555  1.468234e-10  \n",
            "C(treatment, Sum):C(device_type, Sum)                9.998638  1.570166e-03  \n",
            "C(treatment, Sum):C(region, Sum)                     1.599127  1.714998e-01  \n",
            "C(device_type, Sum):C(region, Sum)                   1.702252  1.464034e-01  \n",
            "C(treatment, Sum):C(device_type, Sum):C(region,...   1.705958  1.455674e-01  \n",
            "Residual                                                  NaN           NaN  \n",
            "count_vdps\n",
            "                                                          sum_sq       df  \\\n",
            "C(treatment, Sum)                                   7.605312e+01      1.0   \n",
            "C(device_type, Sum)                                 2.659117e+02      1.0   \n",
            "C(region, Sum)                                      8.610824e+02      4.0   \n",
            "C(treatment, Sum):C(device_type, Sum)               1.438625e+01      1.0   \n",
            "C(treatment, Sum):C(region, Sum)                    6.030126e+02      4.0   \n",
            "C(device_type, Sum):C(region, Sum)                  4.489693e+02      4.0   \n",
            "C(treatment, Sum):C(device_type, Sum):C(region,...  1.038951e+03      4.0   \n",
            "Residual                                            1.015789e+06  12980.0   \n",
            "\n",
            "                                                           F    PR(>F)  \n",
            "C(treatment, Sum)                                   0.971825  0.324244  \n",
            "C(device_type, Sum)                                 3.397883  0.065303  \n",
            "C(region, Sum)                                      2.750779  0.026572  \n",
            "C(treatment, Sum):C(device_type, Sum)               0.183831  0.668109  \n",
            "C(treatment, Sum):C(region, Sum)                    1.926360  0.103053  \n",
            "C(device_type, Sum):C(region, Sum)                  1.434259  0.219733  \n",
            "C(treatment, Sum):C(device_type, Sum):C(region,...  3.318990  0.010028  \n",
            "Residual                                                 NaN       NaN  \n",
            "sale\n",
            "                                                         sum_sq       df  \\\n",
            "C(treatment, Sum)                                      0.025091      1.0   \n",
            "C(device_type, Sum)                                    0.525294      1.0   \n",
            "C(region, Sum)                                         2.276446      4.0   \n",
            "C(treatment, Sum):C(device_type, Sum)                  0.045137      1.0   \n",
            "C(treatment, Sum):C(region, Sum)                       2.908854      4.0   \n",
            "C(device_type, Sum):C(region, Sum)                     1.355689      4.0   \n",
            "C(treatment, Sum):C(device_type, Sum):C(region,...     2.114709      4.0   \n",
            "Residual                                            1192.525410  12980.0   \n",
            "\n",
            "                                                           F    PR(>F)  \n",
            "C(treatment, Sum)                                   0.273100  0.601269  \n",
            "C(device_type, Sum)                                 5.717540  0.016810  \n",
            "C(region, Sum)                                      6.194475  0.000056  \n",
            "C(treatment, Sum):C(device_type, Sum)               0.491292  0.483364  \n",
            "C(treatment, Sum):C(region, Sum)                    7.915328  0.000002  \n",
            "C(device_type, Sum):C(region, Sum)                  3.688987  0.005252  \n",
            "C(treatment, Sum):C(device_type, Sum):C(region,...  5.754367  0.000127  \n",
            "Residual                                                 NaN       NaN  \n"
          ]
        }
      ],
      "source": [
        "# performing a three-way anova for each KPI \n",
        "KPIs = ['duration', 'count_searches', 'count_vdps', 'sale']\n",
        "\n",
        "for KPI in KPIs:\n",
        "    print(KPI)\n",
        "    three_way_anova(KPI, 'treatment', 'device_type', 'region', user_summary_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Direct Answers to Questions\n",
        "\n",
        "**1) Evaluate the effect of the new feature on engagement (searches and VDPs) and conversion using statistical significance where applicable**\n",
        "## Section 1: User Attributes\n",
        "Users in this dataset have three categorical attributes as Summarized in the table below\n",
        "| attribute | python\\_name | Description |\n",
        "| -- | -- | -- |\n",
        "| Device Type | `device_type` | User's device, can be Desktop or Mobile | \n",
        "| Region | `region` | User's region, can be Midwest, New England, Pacific Northwest, Southeast, Southwest | \n",
        "| Treatment | `treatment` | What testing group the user is in. Can be Test or Control | \n",
        "\n",
        "\n",
        "## Section 2: Description of Metrics and KPIs \n",
        "The metrics used to evaluate the engagement of the feature were:\n",
        " | Metric Name | pd column name | Description | \n",
        " | --------------- | -------------------- | -------- |\n",
        "| Duration | `duration` | The amount of time in minutes that a user spent on their customer journey. |\n",
        "| Searches | `count_searches` | The number of searches performed by the user. | \n",
        "| VDP Views | `vdps` | The number of VDP pages viewed by the user. |\n",
        "| Sale | `sale` | The number of performed by the user |\n",
        "\n",
        "The metrics used to evaluate the conversion attributable to the feature were main raw sales, and sales/user. Being a non-normally distributed value special care had to be taken with this metric.\n",
        "\n",
        "## Section 3: Significance testing for Feature Evaluation\n",
        "### Section 3.1: Student t-testing \n",
        "The student t-test for independent samples was used to evaluate feature engagement. \n",
        "Our goal for the t-test was to compare the means of the populations between test and control groups.\n",
        "The Null Hypothesis, $H_0$ is that the means for the tested KPI are the same between Test and Control groups. The Alternate Hypothesis $H_1$ is that the means for the tested KPI is *different* between Test and Control Groups  \n",
        "#### Table 3.1.1: t-test results over entire dataset\n",
        "| KPI | meanTest | meanCtrl | Tstat | p-value | RejH0@0.05 | \n",
        "| -- | -- | -- | -- | -- | -- |\n",
        "| duration | 16.679894 | 16.563516 | 0.188700  |0.850331 | False | \n",
        "| count_searches |5.620047| 5.338462 |1.775687 |0.075808 | False|\n",
        "| count_vdps|  1.287024 | 1.440366 | -0.987257 | 0.323535 | False| \n",
        "| sale | 0.101632  |0.104494|  -0.536508 | 0.591617 | False |\n",
        "\n",
        "From this test we can see that at the highest level there is no difference between test and control groups. \n",
        "We see a KPI called sale that I have not described. This is the average number of sales per customer. It is also the conversion rate for the entire population. However, given that any given user did not buy more than 1 car, the t-test is not appropriate because the output variable for sale is (1,0). We must employ the Chi-Squared test \n",
        "### Section 3.2: Chi-Squared testing \n",
        "The Chi-Squared test is used to determine whether two categorical variables are related or not. Our goal for the Chi-Squared test was to compare the counts of sales between test and control groups. The null hypothesis, $H_0$ is that there is no difference between the distribution of sales between the Test and Control groups. The alternate hypothesis, $H_1$, is that there *is* a difference between the distributions of the test and control groups. \n",
        "\n",
        "#### Table 3.2.1: Contingency Table over the entire dataset\n",
        "|   | No Sale | Sale | Total |\n",
        "| -- | -- | -- | -- |\n",
        "| Control |  5879 | 686 | 6565 | \n",
        "| Test | 5781 | 654 | 6435 | \n",
        "|Total | 11660 | 1340 | 13000 |   \n",
        "\n",
        "The Null Hypothesis $H_0$ is that there is no difference between the distribution of sales between \n",
        "Test and Control groups. The Alternate Hypothesis $H_1$ is that there *is* a significant difference between Test and Control groups. The p-value observed for this contingency table was $0.612$ which was much larger than the $\\alpha=0.05$  test level, meaning that we failed to reject $H_0$. \n",
        "\n",
        "While these initial tests may not be promising lets see if there is niche population that could benefit from this feature. \n",
        "### Section 3.3: Significance Testing on Grouped Data \n",
        "The groupable attributes of a user are device and region. Lets see what kind of Significance testing results we get for users that are grouped by device type. \n",
        "\n",
        "#### Table 3.3.1: t-test results for Desktop users\n",
        "\n",
        "| KPI | meanTest | meanCtrl  |Tstat | pvalue | RejH0@0.05 |\n",
        "| -- | -- | -- | -- | -- |-- |\n",
        " | duration | 17.311658|  17.162309 | 0.179144 | 0.857830 | False| \n",
        "  | count_searches | 5.424602 | 5.587370 | -0.596829 | 0.550640 | False | \n",
        "  | count_vdps | 1.382789 | 1.589243 | -0.764724 | 0.444460 | False | \n",
        "  | sale | 0.108713 | 0.108643 | 0.009694 | 0.992266 | False | sale |\n",
        "  \n",
        "  From Table 3.3.1 we can see that this feature has no *significant* effect on the user engagement KPIs for Desktop users.\n",
        "  #### Table 3.3.2: Contingency Table for Desktop users\n",
        "  |   | No Sale | Sale | Total |\n",
        "  | -- | -- | -- | --| \n",
        "  | **Control** | 3331 | 406 | 3737\n",
        "  | **Test** | 3304 | 403 | 3707 |\n",
        "  |**Total** | 6635 | 809 | 7444 |\n",
        "  \n",
        "Following the $H_0$, $H_1$ formats presented in Section 3.2, the Chi-Squared test affords a p-value observed for this contingency table was $1.000$ which was much larger than the $\\alpha=0.05$  test level, meaning that we failed to reject $H_0$. \n",
        "\n",
        "**We can see from this data that Desktop users are unaffected by the Feature.**\n",
        "\n",
        "#### Table 3.3.3: t-test results for Mobile \n",
        "| KPI | meanTest | meanCtrl | Tstat | pvalue | RejH0@0.05 | \n",
        "| -- | -- | -- | -- | -- | -- |\n",
        "| duration | 15.821408 | 15.772254 | 0.053815 | 9.570847e-01 | False |\n",
        "| count_searches | 5.885630 | 5.009547 | 13.761598 | 2.138779e-42 | True | \n",
        "| count_vdps | 1.156891 | 1.243635 | -2.493909 | 1.266336e-02 | True | \n",
        "| sale | 0.092009 | 0.099010 | -0.887252 | 3.749819e-01 | False |\n",
        "\n",
        "From the results shown in Table 3.3.3 we see that for mobile users they had *statistically significant* increases in the number of searches and the number of VDP pages visited per user. However they did not spend any more time on the site.\n",
        "\n",
        "#### Table 3.3.4: Contingency Table for Mobile users\n",
        "|        | No Sale  | Sale | Total |              \n",
        "| -- | -- | -- | -- |\n",
        "| **Control** |  2548 |  280 |  2828 |\n",
        "| **Test**      | 2477 | 251 | 2728 |\n",
        "| **All**       | 5025 | 531 |  5556 |\n",
        "\n",
        "Following the $H_0$, $H_1$ formats presented in Section 3.2, the Chi-Squared test affords a p-value observed for this contingency table was $0.400$ which was much larger than the $\\alpha=0.05$  test level, meaning that we failed to reject $H_0$. \n",
        "\n",
        "**We can see from this data that Mobile users exposed to this feature search and view VDP's more, but neither buy more often nor spend more time on the site.**\n",
        "\n",
        "Lets also try grouping by region to identify market segments where the feature might be successful. Since we also Identified that Mobile might be the better market we can summarize t-test results by regions in  the following tables \n",
        "#### Significant Testing Summary Tables by Region\n",
        "####  Table 1.1: Duration \n",
        "| Region            | % Change | %Ch. Mobile | Significant | Sign. Mobile |\n",
        "| ----------------- | -------- | ----------- | ----------- | ------------ |\n",
        "| Midwest           | \\-36%    | \\-72%       | TRUE        | TRUE         |\n",
        "| New England       | 22%      | 47%         | TRUE        | TRUE         |\n",
        "| Pacific Northwest | 12%      | 16%         | FALSE       | FALSE        |\n",
        "| Southeast         | \\-4%     | 12%         | FALSE       | FALSE        |\n",
        "| Southwest         | 15%      | 26%         | FALSE       | FALSE        |\n",
        "\n",
        "#### Table 1.2 : Number of Searches \n",
        "| Region            | % Change | %Ch. Mobile | Significant | Sign. Mobile |\n",
        "| ----------------- | -------- | ----------- | ----------- | ------------ |\n",
        "| Midwest           | 1%       | \\-2%        | FALSE       | TRUE         |\n",
        "| New England       | 8%       | 20%         | TRUE        | TRUE         |\n",
        "| Pacific Northwest | 11%      | 22%         | TRUE        | TRUE         |\n",
        "| Southeast         | 13%      | 29%         | TRUE        | TRUE         |\n",
        "| Southwest         | \\-8%     | 19%         | FALSE       | TRUE         |\n",
        "\n",
        "#### Table 1.3: Number of VDP visits \n",
        "| Region            | % Change | %Ch. Mobile | Significant | Sign. Mobile |\n",
        "| ----------------- | ----- | ------ | ----- | ---- |\n",
        "| Midwest           | \\-43% | \\-100% | TRUE  | TRUE |\n",
        "| New England       | 15%   | 25%    | TRUE  | TRUE |\n",
        "| Pacific Northwest | 15%   | 25%    | TRUE  | TRUE |\n",
        "| Southeast         | 4%    | 20%    | FALSE | TRUE |\n",
        "| Southwest         | \\-38% | 27%    | FALSE | TRUE |\n",
        "\n",
        "#### Table 1.4: Conversion Rate t-test\n",
        "Note: Conversion rate = (sales/total number of customers in group) percent changes are shown  \n",
        "| Region            | % Change | %Ch. Mobile | Significant | Sign. Mobile |\n",
        "| ----------------- | -------- | ----------- | ----------- | ------------ |\n",
        "| Midwest           | \\-51%    | \\-100%      | TRUE        | TRUE         |\n",
        "| New England       | 29%      | 59%         | FALSE       | TRUE         |\n",
        "| Pacific Northwest | 11%      | 14%         | FALSE       | FALSE        |\n",
        "| Southeast         | \\-11%    | 4%          | FALSE       | FALSE        |\n",
        "| Southwest         | 18%      | 29%         | FALSE       | FALSE        |\n",
        "\n",
        "#### Table 1.5: Sales Chi-Squared test \n",
        "| Region            | Significant | Sign. Mobile |\n",
        "| ----------------- | ----------- | ------------ |\n",
        "| Midwest           | TRUE        | TRUE         |\n",
        "| New England       | FALSE       | TRUE         |\n",
        "| Pacific Northwest | FALSE       | FALSE        |\n",
        "| Southeast         | FALSE       | FALSE        |\n",
        "| Southwest         | FALSE       | FALSE        |\n",
        "\n",
        "### Section 3.4 Three-way ANOVA for Attribute and Feature Evaluation\n",
        "A limitation of a t-test and Chi-Square test is that many individual tests that must be run to evaluate the feature using engagement KPIs. One must also run preliminary tests to conclude that an attribute or subgroup would not benefit from the feature. In addition, Cross effects cannot be analyzed sufficiently. Three-way ANOVA is a way to evaluate categorical variables' (device type, region, treatment) their effect on continuous variables(duration, searches, VDP views, conversion rate). \n",
        "\n",
        "A summary of Two-Way ANOVA can be found on Wikipedia, but I summarize the Three-way ANOVA. I summarize my findings for Three way ANOVA in Table 3.4.1\n",
        "\n",
        "#### Table 3.4.1: Summary of Three-Way ANOVA results\n",
        "| KPI | Influencing Attributes | \n",
        "| -- | -- |\n",
        "| Duration | device type, region | \n",
        "| Searches | region, treatment |\n",
        "| VDP Views | region |   \n",
        "| Conv. Rate | device type, region | \n",
        "\n",
        "From Table 3.4.1 we can easily see which attributes were the key drivers for the variation in the means of the KPI for each group. Surprisingly enough we only see treatment be an driver for variation when it came to searches. From our t-test study we saw differences in means between control and treatment groups. In Table 3.4.2 We examine cross effects between categorical variables.\n",
        "\n",
        "#### Table 3.4.2: Summary of  crossed variable that influence\n",
        "Abbrevaiting device type to D, region to R, and Treatment to T.\n",
        "| KPI | Influencing Crossed Attributes | \n",
        "| -- | -- |\n",
        "| Duration | T $\\times$ R, D $\\times$ R, D $\\times$ R $\\times$ T\n",
        "| Searches | T $\\times$ D\n",
        "| VDP Views | D $\\times$ R $\\times$ T\n",
        "| Conv. Rate | T $\\times$ R, D $\\times$ R, D $\\times$ R $\\times$ T\n",
        "\n",
        "Table 3.4.2 Shows some cross effects. These cross effects largely confirm that device type combined with Region and Treatment are significant sources of variation for the means of each KPI. \n",
        "  \n",
        "**2) Summarize and highlight insights (or issues) in user behavior across various segments**\n",
        "### Insights \n",
        "1. Desktop users were unaffected by the feature, while mobile users had increased engagement when the feature was delpoyed.\n",
        "2. The Midwestern mobile users did not enjoy this feature. \n",
        "3. With the exception of Midwest, mobile users from other regions spent, 25% more time on the site.\n",
        "4. With the exception of Midwest, mobile users from other regions performed 23% more searches. \n",
        "5. With the exception of Midwest, mobile users from other regions viewed 24% more VDP's.\n",
        "6. With the exception of Midwest, mobile users bought 26% more than the control group. \n",
        "\n",
        "### Issues\n",
        "1. Desktop users were unaffected by the feature. \n",
        "2. The feature had no *statistically significant* effect on conversion rate for mobile users in Pacific Northwest, Southeast, and Southwest.\n",
        "3. The feature had no *statistically significant* effect on website duration for mobile users in Pacific Northwest, Southeast and Southwest. \n",
        "4. The feature did not have a *statistically significant* effect on VDP visits for Southeast mobile users.\n",
        "\n",
        "**3) Provide a recommendation on whether or not to permanently deploy the feature to all users, some users, or no users**\n",
        "From the results of the Engagement study, I would first only deploy this change to Mobile users. I would deploy the feature to all regions except Midwest. \n",
        "From the results of the Conversion study, I would definitely deploy the feature in New England, and later deploy in Southwest, Pacific Northwest and Southeast in that order. I would not deploy the feature in the Midwest. \n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Carvana Analytics Assignment -- AB Test",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
